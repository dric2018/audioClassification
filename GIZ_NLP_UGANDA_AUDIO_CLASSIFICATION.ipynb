{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GIZ-NLP-UGANDA-AUDIO-CLASSIFICATION.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1-Hy7pf1ToQ79EyMtLk67XRh0G18uPgRG",
      "authorship_tag": "ABX9TyMzRCD/1LWzxTS6isEQxF52",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dric2018/audioClassification/blob/main/GIZ_NLP_UGANDA_AUDIO_CLASSIFICATION.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynf82MalzP_n",
        "outputId": "17bef854-4a47-4a0f-f652-cd2328ac597a"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1D_IVyo-2h6H",
        "outputId": "7a329b17-c348-42fc-dd04-d15fec0bd8a9"
      },
      "source": [
        "%%writefile init.sh\n",
        "pip install git+https://github.com/eaedk/testing-zindi-package.git -q\n",
        "\n",
        "apt-get install unzip unrar p7zip-full\n",
        "python3 -m pip install patool -q\n",
        "python3 -m pip install pyunpack -q\n",
        "pip install -q torch\n",
        "pip install -q pytorch-lightning \n",
        "pip install librosa\n",
        "pip install efficientnet-pytorch\n",
        "pip install torchaudio\n",
        "pip install -U pandas # upgrade pandas\n",
        "pip install swifter\n",
        "mkdir {/content/models, data/datasets/images}\n",
        "\n",
        "python init.py --username I_am_Zeus_AI --download #Connects the user and download the dataset from zindi\n",
        "unzip -q data/raw/audio_files.zip -d data/\n",
        "unzip -q data/raw/AdditionalUtterances.zip -d data/\n",
        "unzip -q data/raw/nlp_keywords_29Oct2020.zip -d data/\n",
        "\n",
        "python utils.py --data_path /content/data/datasets --csv_path /content/data/Giz-agri-keywords-data --create_train_df True --create_spectrograms True --specs_path /content/data/Giz-agri-keywords-data/datasets\n",
        "python train.py --train_csv_path /content/data/final_train.csv --gpus 1 --test_batch_size 32 --train_batch_size 64 --kfold 3 --num_epochs 50 --img_size 224 --specs_images_path /content/data/datasets/images --save_models_to /content/models --seed_value 2020 --lr 0.0023182567385564073\n",
        "python inference.py --test_csv_path /content/data/final_test.csv --models_path /content/models --sample_csv_path /content/data/SampleSubmission.csv --arch 'resnet34' --save_resulting_file_to /content/ --test_batch_size 16 --specs_images_path /content/data/datasets/images"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing init.sh\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUR3YGI45eyR",
        "outputId": "2b20ac42-c78e-4e2d-8c81-4546b2f99aed"
      },
      "source": [
        "%%writefile init.py\n",
        "\n",
        "import os, sys, gc, glob\n",
        "import argparse\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from zindi import user as zuser\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser(description='Logging phase')\n",
        "\n",
        "parser.add_argument('--username', type=str, help='Your Zindi username')\n",
        "parser.add_argument('--prefix', type=str, default='/content/drive/My Drive/Zindi/GIZ/', help=\"***\")\n",
        "parser.add_argument('--data', type=str, default='data/', help=\"***\")\n",
        "parser.add_argument('--seed', type=int, default=2020, help='randomness factor')\n",
        "parser.add_argument('--download', action='store_true', help=\"\")\n",
        "\n",
        "def download(args):\n",
        "\tuser = zuser.Zindian(args.username)\n",
        "\tuser.which_challenge\n",
        "\tuser.select_a_challenge()\n",
        "\tuser.download_dataset(args.data)\n",
        "\n",
        "def info(args):\n",
        "\ttrain = pd.read_csv(args.prefix + 'BaseTrain.csv')\n",
        "\tfull = pd.read_csv(args.prefix + 'Train.csv')\n",
        "\tadd = pd.read_csv(args.prefix + 'AddTrain.csv')\n",
        "\n",
        "\tlung_words = add.target.unique()\n",
        "\teng_words = [w for w in train.target.unique() if w not in lung_words]\n",
        "\n",
        "\tdicts = {\"base data\": train, \"add data\": add, \"full data\": full}\n",
        "\n",
        "\twith open(args.prefix + \"info.txt\", \"w\") as f:\n",
        "\t\tfor name, df in dicts.items():\n",
        "\t\t\tinfo = f\"##{name}##. \\nIt contains {df.target.nunique()} unique classes .\\n\"\n",
        "\t\t\tinfo += f\"Shape: {df.shape}\\n\\n\"\n",
        "\t\t\tf.write(info)\n",
        "\t\tf.close()\n",
        "  \n",
        "\n",
        "def main(parser):\n",
        "\targs = parser.parse_args()\n",
        "\n",
        "\tif args.download: download(args)\n",
        "\n",
        "\tif args.pp: \n",
        "\t\tpreprocessing(args)\n",
        "\t\tinfo(args)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\tmain(parser)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing init.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_IO4lEA1gP7",
        "outputId": "dd394421-3ce5-4024-bf35-27dabdb77e67"
      },
      "source": [
        "%%writefile utils.py\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "from zipfile import ZipFile\n",
        "from tqdm import tqdm \n",
        "from pyunpack import Archive\n",
        "import argparse\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "import librosa\n",
        "import matplotlib.pyplot as plt \n",
        "import swifter\n",
        "from scipy import signal\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')\n",
        "\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--data_path',  type=str, help='data source directory')\n",
        "parser.add_argument('--destination_path',  type=str, help='data destination directory')\n",
        "parser.add_argument('--extract_files', default=False, type=bool, help='execute extraction or not')\n",
        "parser.add_argument('--kind', default='7z', type=str, help='For .7z files extraction')\n",
        "parser.add_argument('--create_train_df', default=True, type=bool, help='Create a training dataframe or not')\n",
        "parser.add_argument('--csv_path', type=str, help='Csv files path')\n",
        "parser.add_argument('--specs_path', type=str, help='Spectrograms files path')\n",
        "parser.add_argument('--create_spectrograms', default=True, type=bool, help='Create log spectrogram or not')\n",
        "parser.add_argument('--sample_csv_path', type=str, help='sample submission csv file')\n",
        "\n",
        "\n",
        "def extract_files(data_path:str, destination_path:str):\n",
        "    files = os.listdir(data_path)\n",
        "    dest = os.path.join(destination_path)\n",
        "\n",
        "    os.makedirs(dest, exist_ok=True)\n",
        "\n",
        "    for fn in tqdm(files):\n",
        "        if fn.split('.')[-1] == \"zip\":\n",
        "            try :\n",
        "                with ZipFile(os.path.join(data_path, fn), \"r\") as zip_ref:\n",
        "                    for file_ in tqdm(iterable=zip_ref.namelist(), total=len(zip_ref.namelist()), desc=\"Extrating files\"):\n",
        "\n",
        "                        # Extract each file to another directory\n",
        "                        # If you want to extract to current working directory, don't specify path\n",
        "                        zip_ref.extract(member=file_, path=dest)\n",
        "                    print(f'[INFO] successfully extracted files from {fn}')\n",
        "\n",
        "            except Exception as ex:\n",
        "                print(f'[ERROR] {ex}')\n",
        "\n",
        "\n",
        "def extract_files_v1(data_path:str, destination_path:str):\n",
        "    files = os.listdir(data_path)\n",
        "    dest = os.path.join(destination_path)\n",
        "\n",
        "    os.makedirs(dest, exist_ok=True)\n",
        "\n",
        "    for fn in tqdm(files):\n",
        "        if fn.split('.')[-1] == \"7z\":\n",
        "            try :\n",
        "                print(f'[INFO] Extracting files from {fn}')\n",
        "\n",
        "                tqdm(Archive(os.path.join(data_path, fn)).extractall(dest), desc=len(os.listdir(os.path.join(data_path, fn))))\n",
        "                print(f'[INFO] successfully extracted files from {fn}')\n",
        "\n",
        "            except Exception as ex:\n",
        "                print(f'[ERROR] {ex}')\n",
        "\n",
        "\n",
        "\n",
        "def calc_duration(file_path):\n",
        "    signal, sr = librosa.load(file_path)\n",
        "    return signal.shape[0] / sr\n",
        "    \n",
        "\n",
        "def label_to_int(label, class_dict):\n",
        "    return class_dict[label]\n",
        "\n",
        "\n",
        "def create_train_dataframe(csv_path, data_path):\n",
        "    if ('Train.csv' in os.listdir(csv_path))  or ('train.csv' in os.listdir(csv_path)):\n",
        "        try:\n",
        "            df = pd.read_csv(os.path.join(csv_path, 'Train.csv'))\n",
        "        except:\n",
        "            df = pd.read_csv(os.path.join(csv_path, 'train.csv'))\n",
        "\n",
        "\n",
        "        folder_list = os.listdir(data_path)\n",
        "        files_list = []\n",
        "        labels = []\n",
        "\n",
        "        for folder in folder_list:\n",
        "            if folder != 'audio_files':\n",
        "                try:\n",
        "                    keywords = os.listdir(os.path.join(data_path, folder))\n",
        "                    for keyword in keywords:\n",
        "                        files = os.listdir(os.path.join(data_path, folder, keyword))\n",
        "                        files_list += [os.path.join(folder, keyword, fn) for fn in files ]\n",
        "                        labels += [keyword for _ in range(len(os.listdir(os.path.join(data_path, folder, keyword)))) ]\n",
        "                except:\n",
        "                    pass\n",
        "                \n",
        "        \n",
        "\n",
        "        df = df.append(pd.DataFrame({\n",
        "            'fn' : files_list,\n",
        "            'label' : labels\n",
        "        }), ignore_index=True)\n",
        "\n",
        "\n",
        "        df['fn'] = data_path +'/'+ df['fn']\n",
        "        df['duration'] = df.swifter.progress_bar(enable=True, desc='computing audio durations').apply(lambda row : calc_duration(row.fn), axis=1) # use all available cpu cores\n",
        "        df['label'] = df.swifter.progress_bar(enable=True, desc='Converting labels to ints').apply(lambda row : label_to_int(label=row.label, class_dict={l:idx for idx, l in enumerate(df.label.unique().tolist())}) , axis=1)# use all available cpu cores\n",
        "        df.to_csv(os.path.join(csv_path, 'final_train.csv'), index=False)\n",
        "\n",
        "\n",
        "\n",
        "def log_specgram(audio, sample_rate, window_size=20, step_size=10, eps=1e-10):\n",
        "\n",
        "    \"\"\"\n",
        "    Borrowing log spec function from https://www.kaggle.com/davids1992/data-visualization-and-investigation\n",
        "    \"\"\"\n",
        "    nperseg = int(round(window_size * sample_rate / 1e3))\n",
        "    noverlap = int(round(step_size * sample_rate / 1e3))\n",
        "    freqs, _, spec = signal.spectrogram(audio,\n",
        "                                    fs=sample_rate,\n",
        "                                    window='hann',\n",
        "                                    nperseg=nperseg,\n",
        "                                    noverlap=noverlap,\n",
        "                                    detrend=False)\n",
        "    return freqs, np.log(spec.T.astype(np.float32) + eps)\n",
        "\n",
        "\n",
        "def wav2img(wav_path, targetdir='', figsize=(4,4)):\n",
        "    \"\"\"\n",
        "    takes in wave file path\n",
        "    and the fig size. Default 4,4 will make images 288 x 288\n",
        "    \"\"\"\n",
        "    fig = plt.figure(figsize=figsize)    \n",
        "    # use soundfile library to read in the wave files\n",
        "    sound, samplerate  = librosa.load(wav_path)\n",
        "    _, spectrogram = log_specgram(sound, samplerate)\n",
        "    \n",
        "    ## create output path\n",
        "    output_file = wav_path.split('/')[-1].split('.wav')[0]\n",
        "    output_file = targetdir +'/'+ output_file\n",
        "    #plt.imshow(spectrogram.T, aspect='auto', origin='lower')\n",
        "    plt.imsave('%s.png' % output_file, spectrogram)\n",
        "    plt.close()\n",
        "\n",
        "    return output_file+'.png'\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    if args.extract_files:\n",
        "        if args.kind == 'zip':\n",
        "            try:\n",
        "                extract_files(args.data_path, args.destination_path)\n",
        "            except Exception as ex:\n",
        "                raise ex\n",
        "        else:\n",
        "            try:\n",
        "                extract_files_v1(args.data_path, args.destination_path)\n",
        "            except Exception as ex:\n",
        "                raise ex\n",
        "\n",
        "    if args.create_train_df:\n",
        "        try:\n",
        "            df = create_train_dataframe(args.csv_path, args.data_path)\n",
        "        except Exception as ex:\n",
        "            print(ex)\n",
        "\n",
        "    if args.create_spectrograms:\n",
        "        try:\n",
        "            # train spectrograms\n",
        "            img_dir = args.specs_path+'/images'\n",
        "            df['spec_path'] = img_dir\n",
        "            os.makedirs(img_dir, exist_ok=True)\n",
        "            for row in tqdm(df.iterrows(), total=len(df), desc='Creating specs'):\n",
        "                output_file = wav2img(wav_path=row[1].fn, targetdir=img_dir)\n",
        "                df.at[row[0], 'spec_path'] = output_file\n",
        "\n",
        "            # save dataframe with specs paths\n",
        "            df.to_csv(os.path.join(args.csv_path, 'final_train.csv'), index=False)\n",
        "            \n",
        "            # test spectrograms\n",
        "            sample = pd.read_csv(args.sample_csv_path)\n",
        "            sample['fn'] = args.data_path +'/'+ sample['fn']\n",
        "            sample['spec_path'] = img_dir\n",
        "\n",
        "            for row in tqdm(sample.iterrows(), total=len(sample), desc='Creating specs'):\n",
        "                output_file = wav2img(wav_path=row[1].fn, targetdir=img_dir)\n",
        "                sample.at[row[0], 'spec_path'] = output_file\n",
        "\n",
        "            sample.to_csv(os.path.join(args.csv_path, 'final_test.csv'), index=False)\n",
        "            \n",
        "        except:\n",
        "            pass"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing utils.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "moyLKz2m8QPq",
        "outputId": "409f64dc-d68a-4147-f3f7-47b857b95488"
      },
      "source": [
        "%%writefile datasets.py\n",
        "import torch \n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from keras.utils import to_categorical\n",
        "import librosa\n",
        "import os\n",
        "import albumentations as al\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import pandas as pd \n",
        "from pytorch_lightning import seed_everything\n",
        "import numpy as np\n",
        "\n",
        "class AudioDataset(Dataset):\n",
        "    def __init__(self, images_path:str,df:pd.DataFrame, transforms=None,  task = 'train', num_classes=193, one_hot=False, **kwargs):\n",
        "        super(AudioDataset, self).__init__()\n",
        "\n",
        "        self.task = task \n",
        "        self.df = df\n",
        "        try:\n",
        "            self.class_dict = {label:idx for idx,label in enumerate(self.df.label.unique().tolist())}\n",
        "        except:\n",
        "            pass\n",
        "        self.transforms = transforms\n",
        "        self.one_hot = one_hot\n",
        "        self.images_path = images_path\n",
        "        self.num_classes = num_classes\n",
        "        self.log_specs = os.listdir(self.images_path)\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        wav_path = self.df.iloc[index].fn\n",
        "        file_ = wav_path.split('/')[-1].split('.wav')[0]\n",
        "        file_path = self.images_path +'/'+ file_ +'.png'\n",
        "        \n",
        "        # load spectrogram\n",
        "        img = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)        \n",
        "        if self.transforms is not None:\n",
        "            img = self.transforms(image=img)['image']\n",
        "        \n",
        "        sample = {'image' : torch.tensor(img, dtype=torch.float)}\n",
        "\n",
        "        if self.task == 'train':\n",
        "            label = self.df.iloc[index].label\n",
        "            if self.one_hot:\n",
        "                sample.update({\n",
        "                    'label' : torch.tensor(to_categorical(label, self.num_classes), dtype=torch.float)\n",
        "                })            \n",
        "            else:\n",
        "                sample.update({\n",
        "                    'label' : torch.tensor(label, dtype=torch.long)\n",
        "                })\n",
        "        return sample\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing datasets.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmZrm3NS8Q7W",
        "outputId": "d68abdf1-c871-4cad-ad9a-7bc02b6cf541"
      },
      "source": [
        "%%writefile models.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn \n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import models, transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd \n",
        "import random\n",
        "import albumentations as al\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning import Trainer, loggers, seed_everything \n",
        "from efficientnet_pytorch import EfficientNet \n",
        "\n",
        "from datasets import AudioDataset\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class AudioClassifier(pl.LightningModule):\n",
        "    def __init__(self, pretrained=True, out_size=193,img_size=224, lr=0.0023182567385564073, arch_name='resnet34'):\n",
        "        super(AudioClassifier, self).__init__()\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "        if 'efficient' in self.hparams.arch_name:\n",
        "            self.arch = Efficientnet.from_pretrained(self.hparams.arch_name)\n",
        "\n",
        "            #change firs conv layer to accept grayscale images\n",
        "            head = torch.nn.Conv2d(1, 64, kernel_size=(7,7), stride=(2,2), padding=(3,3))\n",
        "            head.weight = torch.nn.Parameter(self.arch.conv1.weight.sum(dim=1, keepdim=True))\n",
        "            self.conv1 = head\n",
        "\n",
        "            # add our own  classifier \n",
        "            self.num_last_ftrs = getattr(self.arch, 'fc').in_features\n",
        "            self.arch.fc = nn.Sequential(\n",
        "                nn.Dropout(.5),\n",
        "                nn.Linear(self.num_last_ftrs, out_size)\n",
        "            ) \n",
        "            torch.nn.init.xavier_normal_(self.arch.fc[1].weight)\n",
        "\n",
        "        else:\n",
        "            self.arch = getattr(models, arch_name)(pretrained)\n",
        "\n",
        "            head = torch.nn.Conv2d(1, 64, kernel_size=(7,7), stride=(2,2), padding=(3,3))\n",
        "            head.weight = torch.nn.Parameter(self.arch.conv1.weight.sum(dim=1, keepdim=True))\n",
        "\n",
        "            self.arch.conv1 = head\n",
        "            # classifier part\n",
        "            self.num_last_ftrs = getattr(self.arch, 'fc').in_features\n",
        "            self.arch.fc = nn.Sequential(\n",
        "                nn.Dropout(.5),\n",
        "                nn.Linear(self.num_last_ftrs, out_size)\n",
        "            ) \n",
        "            torch.nn.init.xavier_normal_(self.arch.fc[1].weight)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.arch(x.view(-1, 1, self.hparams.img_size, self.hparams.img_size))\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        opt = torch.optim.SGD(self.parameters(), lr=self.hparams.lr)\n",
        "        return opt\n",
        "\n",
        "\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch['image'], batch['label']\n",
        "        logits = self(x)\n",
        "\n",
        "        logLoss = self.get_loss(logits=logits, targets=y)\n",
        "        acc = self.get_acc(logits=logits, targets=y)\n",
        "\n",
        "        # logging \n",
        "        self.log('train_acc', acc, on_epoch=True, on_step=False, prog_bar=True)\n",
        "        self.log('train_logLoss', logLoss, on_epoch=True, on_step=True, prog_bar=False)\n",
        "\n",
        "        return {'loss':logLoss, 'train_logloss':logLoss, 'train_acc':acc}\n",
        "\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch['image'], batch['label']\n",
        "        logits = self(x)\n",
        "\n",
        "        val_loss = self.get_loss(logits=logits, targets=y)\n",
        "        val_acc = self.get_acc(logits=logits, targets=y)\n",
        "\n",
        "        # logging \n",
        "        self.log('val_acc', val_acc, on_epoch=True, on_step=False, prog_bar=True)\n",
        "        self.log('val_logLoss', val_loss, on_epoch=True, on_step=False, prog_bar=True)\n",
        "\n",
        "        return {'val_logLoss':val_loss, 'val_acc':val_acc}\n",
        "\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        x, y = batch['image'], batch['label']\n",
        "        logits = self(x)\n",
        "\n",
        "        test_loss = self.get_loss(logits=logits, targets=y)\n",
        "        test_acc = self.get_acc(logits=logits, targets=y)\n",
        "\n",
        "        # logging \n",
        "        self.log('test_acc', test_acc, on_epoch=True, on_step=False, prog_bar=True)\n",
        "        self.log('test_logLoss', test_loss, on_epoch=True, on_step=False, prog_bar=True)\n",
        "\n",
        "        return {'test_logLoss':test_loss, 'test_acc':test_acc}\n",
        "\n",
        "\n",
        "\n",
        "    def get_acc(self, logits, targets):\n",
        "\n",
        "        preds = nn.functional.softmax(logits, dim=1).argmax(1)\n",
        "\n",
        "        acc = (preds == targets).float().mean()\n",
        "        return acc\n",
        "\n",
        "\n",
        "    def get_loss(self, logits, targets):\n",
        "        \n",
        "        loss = nn.CrossEntropyLoss()(logits, targets)\n",
        "        return loss\n",
        "\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing models.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldC43HEv8Rbv",
        "outputId": "e708cca9-0960-494d-ffcd-7a770a99cb96"
      },
      "source": [
        "%%writefile train.py\n",
        "import torch\n",
        "import torch.nn as nn \n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import models, transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split\n",
        "import numpy as np\n",
        "import gc\n",
        "import pandas as pd \n",
        "import random\n",
        "from models import AudioClassifier\n",
        "from datasets import AudioDataset\n",
        "import albumentations as al\n",
        "import os\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning import Trainer, loggers, seed_everything \n",
        "import argparse\n",
        "\n",
        "\n",
        "# arguments parser config\n",
        "parser = argparse.ArgumentParser()\n",
        "\n",
        "parser.add_argument('--train_csv_path', type=str, help='Train csv file')\n",
        "parser.add_argument('--lr', type=float, default=0.0023182567385564073,  help='Learning rate for model training')\n",
        "parser.add_argument('--gpus', type=int, default=1,  help='Number of gpus to use for training')\n",
        "parser.add_argument('--kfold', type=int, default=5,  help='number of folds to use for cross validation')\n",
        "parser.add_argument('--train_batch_size', type=int, default=16,  help='Training batch size')\n",
        "parser.add_argument('--test_batch_size', type=int, default=16,  help='Test/Evaluation batch size')\n",
        "parser.add_argument('--num_epochs', type=int, default=40,  help='Number of epochs for training')\n",
        "parser.add_argument('--img_size', type=int, default=224,  help='input image size')\n",
        "parser.add_argument('--seed_value', type=int, default=2020,  help='Seed value for reproducibility')\n",
        "parser.add_argument('--specs_images_path', type=str, help='Direcetory containing log spectrograms images')\n",
        "parser.add_argument('--save_models_to', type=str, help='Directory to save trained models to')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def make_folds(data:pd.DataFrame, args, n_folds = 10, target_col='label'):\n",
        "  data['fold'] = 0\n",
        "\n",
        "  fold = StratifiedKFold(n_splits = n_folds, random_state=args.seed_value)\n",
        "  for i, (tr, vr) in enumerate(fold.split(data, data[target_col])):\n",
        "    data.loc[vr, 'fold'] = i\n",
        "\n",
        "  return data, n_folds\n",
        "\n",
        "\n",
        "def run_fold(fold, train_df, args,size=(224, 224), arch='resnet18', pretrained=True,   path='MODELS/', data_transforms=None):\n",
        "  \n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  fold_train = train_df[train_df.fold != fold].reset_index(drop=True)\n",
        "  fold_val = train_df[train_df.fold == fold].reset_index(drop=True)\n",
        "\n",
        "  train_ds = AudioDataset(images_path=args.specs_images_path, df=fold_train, transforms=data_transforms['train'])\n",
        "  val_ds = AudioDataset(images_path=args.specs_images_path, df=fold_val, transforms=data_transforms['train'])\n",
        "\n",
        "  trainloader = DataLoader(train_ds, batch_size=args.train_batch_size, shuffle=True , num_workers=os.cpu_count())\n",
        "  validloader = DataLoader(val_ds, batch_size=args.test_batch_size, shuffle=False , num_workers=os.cpu_count())\n",
        "\n",
        "  del train_ds\n",
        "  del val_ds\n",
        "  del fold_train\n",
        "  del fold_val\n",
        "\n",
        "  model = AudioClassifier(arch_name=arch, lr=args.lr, pretrained=pretrained)\n",
        "\n",
        "  tb_logger = loggers.TensorBoardLogger(save_dir='./runs', name='ZINDI-GIZ-NLP-AGRI-KEYWORDS', version=fold)\n",
        "\n",
        "  ckpt_callback = pl.callbacks.ModelCheckpoint(filename=f'ZINDI-GIZ-NLP-AGRI-KEYWORDS-{model.hparams.arch_name}-{fold}-based', \n",
        "                                               dirpath=path, \n",
        "                                               monitor='val_logLoss', \n",
        "                                               mode='min')\n",
        "  \n",
        "  trainer = Trainer(max_epochs=args.num_epochs, gpus=args.gpus, logger=tb_logger, callbacks=[ckpt_callback])\n",
        "\n",
        "  trainer.fit(model, trainloader, validloader)\n",
        "\n",
        "\n",
        "  gc.collect() # collect garbage\n",
        "\n",
        "  return trainer.logged_metrics\n",
        "\n",
        "\n",
        "\n",
        "if __name__=='__main__':\n",
        "\n",
        "  args = parser.parse_args()\n",
        "\n",
        "  _ = seed_everything(args.seed_value)\n",
        "  # data augmentations\n",
        "  data_transforms = {\n",
        "      'train': al.Compose([\n",
        "              al.Resize(args.img_size, args.img_size),\n",
        "              al.Cutout(p=.6, max_h_size=15, max_w_size=10, num_holes=4),\n",
        "              al.Rotate(limit=35, p=.04),\n",
        "              al.Normalize((0.1307,), (0.3081,))\n",
        "      ]),\n",
        "\n",
        "      'test': al.Compose([\n",
        "              al.Resize(args.img_size, args.img_size),\n",
        "              al.Cutout(p=.6, max_h_size=15, max_w_size=10, num_holes=4),\n",
        "              al.Normalize((0.1307,), (0.3081,))\n",
        "      ])\n",
        "  }\n",
        "\n",
        "  df = pd.read_csv(args.train_csv_path)\n",
        "  train, n_folds = make_folds(n_folds=args.kfold, args=args, data=df)\n",
        "  \n",
        "  # traiining loop\n",
        "  best_fold = 0\n",
        "  avg_log_loss = 0.0\n",
        "  best_logloss = np.inf\n",
        "\n",
        "  for fold in range(n_folds):\n",
        "\n",
        "    print('')\n",
        "    print('*'*18)\n",
        "    print(f'Training on fold {fold}')\n",
        "    print('*'*18)\n",
        "    metrics = run_fold(fold=fold, train_df=train, args=args ,size=(224, 224), arch='resnet34', pretrained=True,   path=args.save_models_to, data_transforms=data_transforms)\n",
        "    \n",
        "    print(metrics)\n",
        "    break\n",
        "    print('')\n",
        "    print('*'*75)\n",
        "    print(f'\\t\\t Results for Fold {fold}')\n",
        "    print('-'*75)\n",
        "\n",
        "    print(f'> Train Acc : \\t{train_acc} \\t| Valid Acc : {val_acc}')\n",
        "    print(f'> Train logloss : {train_loss} \\t| Valid logloss : {val_loss}')\n",
        "    print('-'*75)\n",
        "    print(f'\\t\\t Results for Fold {fold}')\n",
        "    print('*'*75)\n",
        "    if metrics['val_logLoss'] < best_logloss:\n",
        "        best_logloss = metrics['val_logLoss']\n",
        "        best_fold = fold\n",
        "        avg_log_loss += metrics['val_logLoss']\n",
        "    else:\n",
        "        avg_log_loss += metrics['val_logLoss']\n",
        "\n",
        "  print(f'[INFO] raining done ! Avg LogLoss : {avg_log_loss / n_folds}')\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing train.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4jgYVnbJ8Rd-",
        "outputId": "7ec4a2e7-d0f1-4a98-a841-b2c00671629e"
      },
      "source": [
        "%%writefile inference.py\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn \n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import models, transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd \n",
        "import random\n",
        "import albumentations as al\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning import Trainer, loggers, seed_everything \n",
        "from efficientnet_pytorch import EfficientNet \n",
        "\n",
        "from datasets import AudioDataset\n",
        "from models import AudioClassifier\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')\n",
        "import argparse\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# arguments parser config\n",
        "parser = argparse.ArgumentParser()\n",
        "\n",
        "parser.add_argument('--test_csv_path', type=str, help='test csv file')\n",
        "parser.add_argument('--sample_csv_path', type=str, help='sample submission csv file')\n",
        "parser.add_argument('--gpus', type=int, default=1,  help='Number of gpus to use for inference')\n",
        "parser.add_argument('--train_batch_size', type=int, default=64,  help='batch size used for training')\n",
        "parser.add_argument('--test_batch_size', type=int, default=16,  help='Test/Evaluation batch size')\n",
        "parser.add_argument('--n_tta', type=int, default=3,  help='Number of Test time Augmentations (TTA)')\n",
        "parser.add_argument('--kfold', type=int, default=3,  help='Number of folds used for training')\n",
        "parser.add_argument('--img_size', type=int, default=224,  help='input image size')\n",
        "parser.add_argument('--seed_value', type=int, default=2020,  help='Seed value for reproducibility')\n",
        "parser.add_argument('--specs_images_path', type=str, help='Direcetory containing log spectrograms images')\n",
        "parser.add_argument('--save_resulting_file_to', type=str, help='Directory to save predictions file')\n",
        "parser.add_argument('--arch', type=str, help='Model architecture to load for inference')\n",
        "parser.add_argument('--num_epochs', type=int, default=40,  help='Number of epochs for training')\n",
        "parser.add_argument('--models_path', type=str, help='Direcetory containing models checkpoints')\n",
        "parser.add_argument('--lr', type=float, default=0.013182567385564073,  help='Learning rate for model training')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def load_models(models_path, arch=None, n_folds=3, device='cuda'):\n",
        "\n",
        "    models = []\n",
        "    for i in range(n_folds):\n",
        "        models.append( AudioClassifier(arch_name=arch) )\n",
        "        models[i].to(device)\n",
        "        try:\n",
        "            models[i].load_from_checkpoint(os.path.join(models_path, f'ZINDI-GIZ-NLP-AGRI-KEYWORDS-{arch}-{i}-based.ckpt'))\n",
        "        except:\n",
        "            models[i].load_from_checkpoint(os.path.join(models_path, f'ZINDI-GIZ-NLP-AGRI-KEYWORDS-{arch}-{i}-based-v0.ckpt'))\n",
        "        models[i].eval()\n",
        "\n",
        "    return models\n",
        "\n",
        "\n",
        "\n",
        "def predict(test_df, images_path, batch_size=16, n_folds=3, transforms=None, n_tta=3, device='cuda', models=None):\n",
        "    # create test AudioDataset\n",
        "    test_ds = AudioDataset(images_path=images_path, task='test', df=test_df, transforms=transforms)\n",
        "    test_dl = DataLoader(dataset=test_ds, shuffle=False, batch_size=batch_size)\n",
        "\n",
        "    predictions_labels = []\n",
        "    predictions_proba = []\n",
        "\n",
        "    out = None\n",
        "\n",
        "    for data in tqdm(test_dl):\n",
        "        x = data['image'].to(device)\n",
        "\n",
        "        for i in range(n_folds):\n",
        "            if i == 0: out = models[i](x)\n",
        "            else: out += models[i](x)\n",
        "\n",
        "        out /= n_folds\n",
        "        out = F.softmax(input=out, dim=1)\n",
        "        out_labels = out.argmax(1)\n",
        "        out_probas = out.detach().cpu().numpy()\n",
        "\n",
        "        \n",
        "        predictions_labels += out_labels.tolist()\n",
        "        predictions_proba += out_probas.tolist()\n",
        "\n",
        "    return predictions_labels ,predictions_proba\n",
        "\n",
        "\n",
        "\n",
        "def make_submission_file(sub:pd.DataFrame,predictions_proba=None, submissions_folder=None, params=None):\n",
        "    submission = pd.DataFrame()\n",
        "    words = sub.columns[2:]\n",
        "    submission['fn'] = sub['fn']\n",
        "    for i, label in enumerate(words):\n",
        "        submission[label] = 0.\n",
        "    for i, label in enumerate(words):\n",
        "        submission.loc[:,label] = np.array(predictions_proba)[:,i]\n",
        "\n",
        "    train_batch_size,_, n_folds, img_size, n_epochs, arch = params.values()\n",
        "\n",
        "    csv_file = f'GIZ_SIZE_{img_size}_arch_{arch}_n_folds_{n_folds}_num_epochs_{n_epochs}_train_bs_{train_batch_size}.csv'\n",
        "    submission.to_csv(os.path.join(submissions_folder, csv_file), index=False)\n",
        "\n",
        "    print(f'[INFO] Submission file save to {os.path.join(submissions_folder, csv_file)}')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    _ = seed_everything(args.seed_value)\n",
        "    # data augmentations\n",
        "    data_transforms = {\n",
        "        'train': al.Compose([\n",
        "                al.Resize(args.img_size, args.img_size),\n",
        "                al.Cutout(p=.6, max_h_size=15, max_w_size=10, num_holes=4),\n",
        "                al.Rotate(limit=35, p=.04),\n",
        "                al.Normalize((0.1307,), (0.3081,))\n",
        "        ]),\n",
        "\n",
        "        'test': al.Compose([\n",
        "                al.Resize(args.img_size, args.img_size),\n",
        "                al.Cutout(p=.6, max_h_size=15, max_w_size=10, num_holes=4),\n",
        "                al.Normalize((0.1307,), (0.3081,))\n",
        "        ])\n",
        "    }\n",
        "\n",
        "    test = pd.read_csv(args.test_csv_path)\n",
        "    sample = pd.read_csv(args.sample_csv_path)\n",
        "\n",
        "    # load models\n",
        "    models = load_models(models_path=args.models_path, n_folds=args.kfold, arch=args.arch)\n",
        "    # make predictions\n",
        "    predictions_labels, predictions_proba = predict(test_df=test, \n",
        "                                                    images_path=args.specs_images_path,\n",
        "                                                    batch_size=args.test_batch_size, \n",
        "                                                    n_folds=args.kfold, \n",
        "                                                    transforms=data_transforms['test'], \n",
        "                                                    n_tta=args.n_tta, \n",
        "                                                    device='cuda', \n",
        "                                                    models=models)\n",
        "\n",
        "    params = {\n",
        "        'train_batch_size': args.train_batch_size,\n",
        "        'test_batch_size':args.test_batch_size,\n",
        "        'kfold': args.kfold, \n",
        "        'img_size': args.img_size,\n",
        "        'epochs': args.num_epochs,\n",
        "        'arch' : args.arch\n",
        "    }\n",
        "\n",
        "    make_submission_file(sub=sample,predictions_proba=predictions_proba, submissions_folder=args.save_resulting_file_to, params=params)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing inference.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9Knsz0A0o_w"
      },
      "source": [
        "data_dir = "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}